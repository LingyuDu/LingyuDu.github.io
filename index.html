<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Lingyu Du</title>

    <meta name="author" content="Lingyu Du">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Lingyu Du
                </p>
                <p>I am a Ph.D. candidate in the
                        <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/software-technology/embedded-systems">Embedded Systems Group</a>
                        of the <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/computer-science">Computer Science Departments</a> at
                        <a href="https://www.tudelft.nl/">Delft University of Technology</a>, the Netherlands, supervised by
                        <a href="https://guohao.netlify.app/">Dr. Guohao Lan</a> and
                        <a href="https://www.st.ewi.tudelft.nl/koen/index.html">Prof. Dr. Koen Langendoen</a>.

                    I obtained my Master’s and Bachelor’s degrees (<a href="https://future.hit.edu.cn/xyjj/list.htm">Honors Program</a>) in Control Science and Engineering from Harbin Institute of Technology, Harbin, China, in 2021 and 2019, respectively.
                </p>
                <p>
                  My research lies in the intersection of pervasive systems and machine learning, with a focus on enhancing efficiency and trustworthiness of machine learning systems and developing human-centered applications.
                </p>
                <p>
                    <strong><b>Office:</b></strong> 2.W.560, Building 28, Van Mourik Broekmanweg 6, Delft<br>

                    <strong><b>Email:</b></strong> Lingyu.Du [AT] tudelft.nl
                </p>
                  <p style="color: rgb(213,15,15);">
                  <strong><b>I am currently in the job market.</b></strong>
                </p>
                <p style="text-align:center">
                  <a href="mailto:lingyu.du@tudelft.nl">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com.hk/citations?user=3jCOwKkAAAAJ&hl=en&authuser=1">Scholar</a> &nbsp;/
                  <a href="https://www.linkedin.com/in/lingyu-du-060961202/">LinkedIn</a> &nbsp;/&nbsp;&nbsp;
                  <a href="https://github.com/LingyuDu">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%; text-align:right">
                <a href="images/photo.png"><img style="width:75%;max-width:100%;object-fit: cover; margin-right:20px;" alt="profile photo" src="images/photo.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <ul>

                    <li>
                    [2025/8/29] I am invited to be on the TPC of <a href="https://www.date-conference.com/tpc">DATE 2026 (Track A6: Applications of AI Systems)</a>.
                            </li> <p></p>
                    <li>
                    [2025/7/9] One paper is accepted to appear in ACM IMWUT/UbiComp 2025.
                            </li> <p></p>
                    <li>
                    [2025/5/15] I am invited to be on the program committee of <a href="https://www.ubicomp.org/ubicomp-iswc-2025/posters-and-demos/">UbiComp/ISWC 2025 (Posters & Demos)</a>.
                            </li> <p></p>

                    <li>
                    [2025/5/7] I presented our paper at CPS-IOT Week 2025 in Irvine, USA.
                            </li> <p></p>

                    <li>
                    [2025/5/6] Our paper <a href="https://dl.acm.org/doi/pdf/10.1145/3736767"><span class="papertitle">EfficientGaze</span></a> is accepted by <strong><b>ACM TOSN</b></strong>.
                            </li> <p></p>
                  <li>
                    [2025/2/25] Our paper <a href="https://dl.acm.org/doi/pdf/10.1145/3715014.3722071"><span class="papertitle">SecureGaze</span></a> has been accepted to appear in <strong><b>ACM SenSys 2025</b></strong>.
                            </li> <p></p>
                  <li>
                    [2024/10] I presented our work at UbiComp 2024 and had a very nice spring vacation in Australia!
                            </li> <p></p>
                  <li>
                    [2024/8/5] Our paper <a href="https://dl.acm.org/doi/pdf/10.1145/3678595">  <span class="papertitle">PrivateGaze</span></a> has been accepted to appear in <strong><b>ACM IMWUT/UbiComp 2024</b></strong>.
                            </li> <p></p>
<!--                  <li>-->
<!--                    [2023/7/18] Our paper entitled <em>``FreeGaze: Resource-efficient Gaze Estimation via Frequency-domain Contrastive Learning''</em> has been accepted to appear in <strong><b>EWSN 2023</b></strong>.-->
<!--                             </li> <p></p>-->
                </ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


   <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()">
      <td style="padding:16px;width:30%;vertical-align:middle">
        <div id="cat4d_image" style="opacity: 1;"> <!-- Visible by default -->
          <img src="images/speakerassistant.png" alt="PrivateGaze Preview" style="width:100%; height:auto;"> <!-- Replaced video with an image -->
        </div>
      </td>
      <td style="padding:8px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2602.01201">
          <span class="papertitle">Talk to Me, Not the Slides: A Real-Time Wearable Assistant for Improving Eye Contact in Presentations</span>
        </a>
        <br>
        <strong>Lingyu Du</strong>, Xucong Zhang, Guohao Lan
        <br>
        <strong><b>arXiv preprint</b></strong>, 2025
        <p></p>
        <p>
          A real-time wearable system that actively assists speakers in maintaining effective eye contact with the audience during live presentations.
        </p>
      </td>
    </tr>

    <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()">
      <td style="padding:16px;width:30%;vertical-align:middle">
        <div id="cat4d_image" style="opacity: 1;"> <!-- Visible by default -->
          <img src="images/efficientgaze.png" alt="PrivateGaze Preview" style="width:100%; height:auto;"> <!-- Replaced video with an image -->
        </div>
      </td>
      <td style="padding:8px;width:70%;vertical-align:middle">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3736767">
          <span class="papertitle">EfficientGaze: Resource-efficient Gaze Estimation via Frequency-domain Multi-task Contrastive Learning</span>
        </a>
        <br>
        <strong>Lingyu Du</strong>, Xucong Zhang, Guohao Lan
        <br>
        <em>ACM Transactions on Sensor Networks</em>, <strong><b>ACM TOSN</b></strong>, 2025,
        <a href="https://github.com/FreeGaze/EfficientGaze">[Project page / Code]</a>
        <p></p>
        <p>
          A resource-efficient gaze estimation framework that significantly reduces the reliance on labeled data and improve the computational efficiency for inference and calibration.
        </p>
      </td>
    </tr>



    <tr onmouseout="private_stop()" onmouseover="private_start()">
      <td style="padding:16px;width:30%;vertical-align:middle">
        <div id="security_video" style="opacity: 1;"> <!-- Default opacity set to 1 -->
          <video width="100%" height="100%" muted autoplay loop>
            <source src="images/SecureGaze.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </td>
      <td style="padding:8px;width:80%; vertical-align:middle">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3715014.3722071">
          <span class="papertitle">SecureGaze: Defending Gaze Estimation Against Backdoor Attacks</span>
        </a>
        <br>
        <strong>Lingyu Du</strong>, Yupei Liu, Jinyuan Jia, Guohao Lan
        <br>
         <em>The 23rd ACM Conference on Embedded Networked Sensor Systems</em><br>
        <strong><b>ACM SenSys</b></strong>, 2025,
        <a href="https://github.com/LingyuDu/SecureGaze">[Project page / Demo / Code]</a>
        <p></p>
        <p>
<!--          The first work to investigate the backdoor vulnerability of gaze estimation models in both the digital world and the physical world, proposing a framework to identify backdoored gaze estimation models and mitigate backdoor behaviors.-->
            The first work that investigates the backdoor vulnerability of gaze estimation models and proposes a framework to identify backdoored gaze estimation models.

        </p>
      </td>
    </tr>

    <tr onmouseout="private_stop()" onmouseover="private_start()">
      <td style="padding:16px;width:30%;vertical-align:middle">
        <div id="private_video" style="opacity: 1;"> <!-- Default opacity set to 1 -->
          <video width="100%" height="100%" muted autoplay loop>
            <source src="images/PrivateGaze.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3678595">
          <span class="papertitle">PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking Services</span>
        </a>
        <br>
        <strong>Lingyu Du</strong>, Jinyuan Jia, Xucong Zhang, Guohao Lan</a>
        <br>
        <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em><br><strong><b>IMWUT/UbiComp</b></strong>, 2024,
        <a href="https://github.com/LingyuDu/PrivateGaze">[Project page / Demo / Code]</a>
        <p></p>
        <p>
<!--          An approach to transform raw images to obfuscated images that do not contain private attributes of users but can be used for effective gaze estimation by black-box models, without the knowledge of structures and parameters of the models. -->
            An approach to transform raw images to obfuscated images that do not contain private attributes of users but can be used for effective gaze estimation by black-box models.

        </p>
      </td>
    </tr>


    <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()">
      <td style="padding:16px;width:30%;vertical-align:middle">
        <div id="cat4d_image" style="opacity: 1;"> <!-- Visible by default -->
          <img src="images/freegaze.png" alt="PrivateGaze Preview" style="width:100%; height:auto;"> <!-- Replaced video with an image -->
        </div>
      </td>
      <td style="padding:8px;width:70%;vertical-align:middle">
        <a href="https://dl.acm.org/doi/10.5555/3639940.3639949">
          <span class="papertitle">FreeGaze: Resource-efficient Gaze Estimation via Frequency-domain Contrastive Learning</span>
        </a>
        <br>
        <strong>Lingyu Du</strong>, Guohao Lan
        <br>
        <em>The 20th International Conference on Embedded Wireless Systems and Networks</em><br> <strong><b>EWSN</b></strong>, 2023,
        <a href="https://github.com/FreeGaze/FreeGaze-Source">[Project page / Code]</a>
        <p></p>
        <p>
          An approach that achieve comparable performance with the supervised baseline, while enabling up to 6.81 and 1.67 times speedup in calibration and inference, respectively.
        </p>
      </td>
    </tr>

<!--        <tr>-->
<!--          <td colspan="2" style="text-align: center;">-->
<!--            <hr style="border: none; height: 1px; background-color: #ccc; width: 95%; margin: 15px auto;">-->
<!--          </td>-->
<!--        </tr>-->

    <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()">
      <td style="padding:16px;width:30%;vertical-align:middle">
        <div id="cat4d_image" style="opacity: 1;"> <!-- Visible by default -->
          <img src="images/EmoOverview.png" alt="PrivateGaze Preview" style="width:100%; height:auto;"> <!-- Replaced video with an image -->
        </div>
      </td>
      <td style="padding:8px;width:70%;vertical-align:middle">
        <a href="https://dl.acm.org/doi/10.1145/3749545">
          <span class="papertitle">Through the Eyes of Emotion: A Multi-faceted Eye Tracking Dataset for Emotion Recognition in Virtual Reality</span>
        </a>
        <br> Tongyun Yang, Bishwas Regmi,
        <strong>Lingyu Du</strong>, Andreas Bulling, Xucong Zhang, Guohao Lan
        <br>
        <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em><br><strong><b>IMWUT/UbiComp</b></strong>, 2025,
        <a href="https://github.com/MultiRepEyeVR/Through-the-Eyes-of-Emotion">[Project page / Code]</a>
        <p></p>
        <p>
            A eye-tracking dataset in VR, combining high-frame-rate periocular videos and high-frequency gaze data to enable accurate, multimodal emotion recognition.         </p>
      </td>
    </tr>

    <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()">
      <td style="padding:16px;width:30%;vertical-align:middle">
        <div id="image" style="opacity: 1;"> <!-- Visible by default -->
          <img src="images/dong7-3047273-large.png" alt="Preview" style="width:100%; height:auto;"> <!-- Replaced video with an image -->
        </div>
      </td>
      <td style="padding:8px;width:70%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/9307268">
          <span class="papertitle">Coordination Strategy of Large-Scale DFIG-Based Wind Farm for Voltage Support With High Converter Capacity Utilization</span>
        </a>
        <br>
          Zhen Dong, Zhongguo Li,
        <strong>Lingyu Du</strong>,
        Yixing Liu, Zhengtao Ding
        <br>
        <em>IEEE Transactions on Sustainable Energy</em>, 2020
        <p></p>
        <p>
<!--           A distributed active power and reactive power coordination scheme for access point voltage support, where a reactive power self-allocation scheme is developed and operated in each WT for reactive power dispatch-->
        A distributed active power and reactive power coordination scheme for access point voltage support.
        </p>
      </td>
    </tr>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;line-height:25px;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Professional Services</h2>
                <ul>
                    <li>
                    <strong>Design, Automation and Test in Europe Conference (DATE)</strong>, Technical Programme Committee, 2026 <br>
                            </li>
                  <li>
                    <strong>UbiComp / ISWC (Posters & Demos)</strong>, Program Committee, 2025 <br>
                            </li>
		          <li>
                    <strong>UbiComp / ISWC (Briefs & Notes)</strong>, Reviewer, 2025 <br>
                            </li>
                  <li>
                    <strong>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</strong>, Reviewer, 2024<br>
                            </li>
                  <li>
                    <strong>ACM Transactions on Sensor Networks (TOSN)</strong>, Reviewer, 2024<br>
                            </li>
                  <li>
                    <strong>IEEE Transactions on Human-Machine Systems</strong>, Reviewer, 2024, 2025<br>
                            </li>
                  <li>
                    <strong>IEEE International Conference on Mobile Ad-Hoc and Smart Systems (MASS)</strong>, Reviewer, 2022, 2023, 2024<br>
                            </li>
                  <li>
                    <strong>Workshop on Smart Wearable Systems and Applications (conjunction with
MobiCom)</strong>, Publicity Chair, 2022, 2025<br>
                            </li>

                </ul>
              </td>
            </tr>
          </tbody></table>


<!--    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;line-height:25px;margin-right:auto;margin-left:auto;"><tbody>-->
<!--              <tr>-->
<!--              <td style="padding:16px;width:100%;vertical-align:middle">-->
<!--                <h2>Supervising Experience</h2>-->
<!--                <ul>-->
<!--                  <li>-->
<!--                    <strong>[CSE 3000] Exploring the Vulnerability of Deep Regression Models to Backdoor Attacks</strong>, 2025, Supervisor<br>-->
<!--                            </li>-->
<!--                  <li>-->
<!--                    <strong>[CSE 3000] Imperceptible Backdoor Attacks on Deep Regression Models</strong>, 2024, Supervisor<br>-->
<!--                            </li>-->

<!--                  <li>-->
<!--                    <strong>[CSE 3000] Deep learning-based Gaze Estimation</strong>, 2023, Supervisor<br>-->
<!--                            </li>-->

<!--                  <li>-->
<!--                    <strong>[CSE 3000] Eye tracking-based Sedentary Activity Recognition with Machine Learning</strong>, 2022, Supervisor<br>-->
<!--                            </li>-->


<!--                </ul>-->
<!--              </td>-->
<!--            </tr>-->
<!--          </tbody></table>-->

<!--    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--              <tr>-->
<!--              <td style="padding:16px;width:100%;vertical-align:middle">-->
<!--                <h2>Education</h2>-->
<!--                <ul>-->
<!--                  <li>-->
<!--                    <strong>Ph.D. candidate</strong>, 2021/9 ~ Now <br>-->
<!--                    Delft University of Technology, Delft, the Netherlands-->
<!--                            </li> <p></p>-->
<!--                  <li>-->
<!--                    <strong>Master of Engineering</strong> in Control Science and Engineering, 2019/9 ~ 2021/7 <br>-->
<!--                    Harbin Institute of Technology, Harbin, China-->
<!--                            </li> <p></p>-->
<!--                  <li>-->
<!--                    <strong>Bachelor of Engineering</strong> in Automation (<a href="https://future.hit.edu.cn/xyjj/list.htm">Honors School</a>), 2015/9 ~ 2019/7 <br>-->
<!--                    Harbin Institute of Technology, Harbin, China-->
<!--                            </li> <p></p>-->

<!--                </ul>-->
<!--              </td>-->
<!--            </tr>-->
<!--          </tbody></table>-->





           



						
						
           

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  This home page is adapted from the template of  <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>

<div style="display: flex; justify-content: center; align-items: center; height: 5vh;">
    <div style="width: 0px; height: 0px; overflow: hidden;">
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=A9ee5Joff_ZY_f35ALe-JrXUQyPM_i1OCuK0wh4A70Y"></script>
    </div>
</div>


